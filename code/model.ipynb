{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据，进行初步的EDA。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/Label/Updated_Medal_Counts_with_Athlete_Numbers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行特征筛选与特征降维\n",
    "- 首先分析变量与目标变量之间的相关性，然后分析变量与目标变量的互信息，综合考虑两者的结果，进行特征选择。\n",
    "- 观察看有没有必要进行特征降维，如果有，可以考虑使用PCA等方法进行特征降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of countries that have never won a medal: 77\n",
      "['CHA', 'NCA', 'LBA', 'PLE', 'COM', 'BRU', 'MDV', 'YAR', 'CGO', 'BEN', 'SOM', 'MLI', 'ANG', 'BAN', 'ESA', 'HON', 'SEY', 'MTN', 'SKN', 'VIN', 'LBR', 'NEP', 'PLW', 'ASA', 'SAM', 'RWA', 'MLT', 'GUI', 'BIZ', 'YMD', 'SLE', 'PNG', 'YEM', 'OMA', 'VAN', 'IVB', 'CAF', 'MAD', 'MAL', 'BIH', 'GUM', 'CAY', 'GBS', 'TLS', 'COD', 'LAO', 'ROT', 'CAM', 'SOL', 'CRT', 'GEQ', 'BOL', 'SAA', 'ANT', 'AND', 'FSM', 'MYA', 'MAW', 'RHO', 'STP', 'LIE', 'GAM', 'COK', 'SWZ', 'NBO', 'ARU', 'NRU', 'VNM', 'BHU', 'MHL', 'KIR', 'UNK', 'TUV', 'NFL', 'SSD', 'LES', 'LBN']\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "target = ['Gold', 'Silver', 'Bronze','Total']\n",
    "features = ['Rank', 'Host','Number_of_people']\n",
    "\n",
    "# # 相关性分析\n",
    "# corr = df[features+target].corr()\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()\n",
    "\n",
    "    \n",
    "# # 统计之前从来没有获得过奖牌的国家\n",
    "medal_0_country = []\n",
    "# for country in df['NOC'].unique():\n",
    "#     country_df = df[df['NOC'] == country]\n",
    "#     if country_df['Goden'].sum() == 0:\n",
    "#         medal_0_country.append(country)\n",
    "        \n",
    "# print('The number of countries that have never won a medal:', len(medal_0_country))\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('../data/summerOly_athletes.csv')\n",
    "#把Medal 这列的数据进行lable编码\n",
    "df1['Medal'] = df1['Medal'].map({'Gold': 3, 'Silver': 2, 'Bronze': 1, 'No medal': 0})\n",
    "for country in df1['NOC'].unique():\n",
    "    country_df = df1[df1['NOC'] == country]\n",
    "    #  观察Medal列，如果这个国家一直都是\"no medal\"，那么这个国家就是从来没有获得过奖牌的国家\n",
    "    if country_df['Medal'].sum() == 0:\n",
    "        medal_0_country.append(country)\n",
    "\n",
    "        \n",
    "        \n",
    "print('The number of countries that have never won a medal:', len(medal_0_country))\n",
    "print(medal_0_country)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来完成第一步也就是零膨胀模型的构建\n",
    "- 首先判断历史数据的平稳性，使用Augmented Dickey-Fuller检验验证奖牌数时间序列的平稳性。\n",
    "- 然后为第一部分（零膨胀逻辑回归）选择合适的特征，使用逻辑回归模型预测某国是否属于“永远无法获奖”的群体（结构性零值）。\n",
    "- 对于第二部分（计数部分），如果数据不平稳就选择负二项分布，如果数据平稳就选择泊松分布。\n",
    "- 最后，将两部分的结果相乘，得到最终的预测结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ZeroInflatedNegativeBinomialP Regression Results                    \n",
      "=========================================================================================\n",
      "Dep. Variable:                       Medal_Count   No. Observations:                 3016\n",
      "Model:             ZeroInflatedNegativeBinomialP   Df Residuals:                     3012\n",
      "Method:                                      MLE   Df Model:                            3\n",
      "Date:                             周日, 26 1月 2025   Pseudo R-squ.:                  0.1605\n",
      "Time:                                   02:46:42   Log-Likelihood:                -5401.7\n",
      "converged:                                  True   LL-Null:                       -6434.2\n",
      "Covariance Type:                       nonrobust   LLR p-value:                     0.000\n",
      "============================================================================================\n",
      "                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "inflate_const               -1.6565      0.260     -6.379      0.000      -2.165      -1.147\n",
      "inflate_Total_appearance    -1.4483      0.173     -8.381      0.000      -1.787      -1.110\n",
      "const                        0.7575      0.051     14.925      0.000       0.658       0.857\n",
      "People_Count                 0.5962      0.076      7.852      0.000       0.447       0.745\n",
      "Last_appearance_athletes    -0.0019      0.063     -0.031      0.975      -0.126       0.122\n",
      "Lag_athletes                 0.6511      0.109      5.965      0.000       0.437       0.865\n",
      "alpha                        1.0342      0.083     12.402      0.000       0.871       1.198\n",
      "============================================================================================\n",
      "{'AND': 0.4744, 'ANG': 0.5118, 'ANT': 0.4723, 'ARU': 0.4477, 'ASA': 0.4444, 'BAN': 0.4488, 'BEN': 0.4825, 'BHU': 0.4454, 'BIH': 0.4401, 'BIZ': 0.4672, 'BOL': 0.4973, 'BRU': 0.3888, 'CAF': 0.466, 'CAM': 0.4549, 'CAY': 0.4672, 'CGO': 0.486, 'CHA': 0.4697, 'COD': 0.4605, 'COK': 0.4487, 'COM': 0.4057, 'ESA': 0.4904, 'FSM': 0.3897, 'GAM': 0.4582, 'GBS': 0.413, 'GEQ': 0.442, 'GUI': 0.5139, 'GUM': 0.45, 'HON': 0.487, 'IVB': 0.441, 'KIR': 0.3603, 'LAO': 0.4515, 'LBA': 0.4644, 'LBN': 0.2758, 'LBR': 0.4858, 'LES': 0.4686, 'LIE': 0.4963, 'MAD': 0.4965, 'MAW': 0.4539, 'MDV': 0.4367, 'MHL': 0.344, 'MLI': 0.5213, 'MLT': 0.5004, 'MTN': 0.4479, 'MYA': 0.4984, 'NCA': 0.4883, 'NEP': 0.489, 'NRU': 0.4056, 'OMA': 0.4457, 'PLE': 0.4135, 'PLW': 0.3828, 'PNG': 0.4768, 'RWA': 0.4648, 'SAM': 0.4964, 'SEY': 0.4533, 'SKN': 0.4078, 'SLE': 0.4675, 'SOL': 0.4441, 'SOM': 0.451, 'SSD': 0.304, 'STP': 0.4066, 'SWZ': 0.4528, 'TLS': 0.3662, 'TUV': 0.3323, 'VAN': 0.4468, 'VIN': 0.4446, 'YEM': 0.4207}\n",
      "最可能首次获奖国家数量：30\n",
      "97.5% 置信区间：[22, 22]\n",
      "预测可能命中国家： ['AND', 'ANG', 'ANT', 'ARU', 'ASA', 'BAN', 'BEN', 'BHU', 'BIH', 'BIZ', 'BOL', 'BRU', 'CAF', 'CAM', 'CAY', 'CGO', 'CHA', 'COD', 'COK', 'COM', 'ESA', 'FSM', 'GAM', 'GBS', 'GEQ', 'GUI', 'GUM', 'HON', 'IVB', 'KIR', 'LAO', 'LBA', 'LBN', 'LBR', 'LES', 'LIE', 'MAD', 'MAW', 'MDV', 'MHL', 'MLI', 'MLT', 'MTN', 'MYA', 'NCA', 'NEP', 'NRU', 'OMA', 'PLE', 'PLW', 'PNG', 'RWA', 'SAM', 'SEY', 'SKN', 'SLE', 'SOL', 'SOM', 'SSD', 'STP', 'SWZ', 'TLS', 'TUV', 'VAN', 'VIN', 'YEM']\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.count_model import ZeroInflatedNegativeBinomialP\n",
    "\n",
    "# Load the data\n",
    "df_score = pd.read_excel('E:/python/2025-MCM-ICM/data/Final_csv/result2.xlsx')\n",
    "#去除\"Unnamed\"列\n",
    "# df_score = df_score.loc[:, ~df_score.columns.str.contains('^Unnamed','Rank')]\n",
    "# print(df_score.head())\n",
    "\n",
    "train_df = df_score[df_score['Year'] < 2024]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#定义模型特征，对于零膨胀特征，我们使用Never_won_medal,Total_appearance,'Athletes_growth_rate', 'Total_appearance' 'Lag_athletes', 'Lag_events'Last_appearance_athletes', 'Last_appearance_events\n",
    "# Select features with less correlation\n",
    "# Define features for inflation and count models\n",
    "infl_features  = ['Total_appearance','Lag_athletes']\n",
    "target = 'Medal_Count'\n",
    "count_features = ['People_Count','Last_appearance_athletes','Lag_athletes']\n",
    "\n",
    "# Remove duplicate features between infl_features and count_features\n",
    "count_features = [x for x in count_features if x not in infl_features]\n",
    "\n",
    "# Check correlations\n",
    "correlation_matrix = train_df[infl_features + count_features].corr()\n",
    "# print(\"Feature correlations:\\n\", correlation_matrix)\n",
    "\n",
    "# Find highly correlated feature pairs (above 0.7)\n",
    "high_correlation = correlation_matrix.abs() > 0.7\n",
    "# Get upper triangle of correlations\n",
    "high_correlation = high_correlation.where(np.triu(np.ones(high_correlation.shape), k=1).astype(bool))\n",
    "# Find feature pairs where correlation is > 0.7\n",
    "high_corr_pairs = np.where(high_correlation)\n",
    "high_corr_values = [(correlation_matrix.index[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]) \n",
    "                    for i, j in zip(*high_corr_pairs) if correlation_matrix.iloc[i, j] > 0.7]\n",
    "# print(\"\\nHighly correlated feature pairs:\")\n",
    "# for feat1, feat2, corr in high_corr_values:\n",
    "#     print(f\"{feat1} - {feat2}: {corr:.3f}\")\n",
    "# print(\"Highly correlated feature pairs:\\n\", high_correlation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create separate scalers for inflation and count features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "infl_scaler = StandardScaler()\n",
    "count_scaler = StandardScaler()\n",
    "\n",
    "# Prepare training data\n",
    "train_infl = train_df[infl_features].copy()\n",
    "train_count = train_df[count_features].copy()\n",
    "\n",
    "# Standardize features\n",
    "train_infl_scaled = infl_scaler.fit_transform(train_infl)\n",
    "train_count_scaled = count_scaler.fit_transform(train_count)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "train_infl_scaled = pd.DataFrame(train_infl_scaled, columns=infl_features, index=train_infl.index)\n",
    "train_count_scaled = pd.DataFrame(train_count_scaled, columns=count_features, index=train_count.index)\n",
    "\n",
    "train_y = train_df[target]\n",
    "\n",
    "# Add constant terms for training data\n",
    "train_count_scaled = pd.DataFrame(train_count_scaled, columns=count_features)\n",
    "train_infl_scaled = pd.DataFrame(train_infl_scaled, columns=infl_features)\n",
    "train_count_scaled = sm.add_constant(train_count_scaled)\n",
    "train_infl_scaled = sm.add_constant(train_infl_scaled)\n",
    "\n",
    "# Train ZINB model with standardized features\n",
    "zinb = ZeroInflatedNegativeBinomialP(\n",
    "    train_y,\n",
    "    train_count_scaled,\n",
    "    exog_infl=train_infl_scaled,\n",
    "    inflation='logit'\n",
    ")\n",
    "\n",
    "# Use robust optimization method\n",
    "result = zinb.fit(maxiter=10000, method='bfgs', disp=0)\n",
    "print(result.summary())\n",
    "\n",
    "#准备预测数据，使用2024年的数据\n",
    "test_df = df_score[df_score['Year'] == 2024]\n",
    "test_infl = test_df[infl_features].copy()\n",
    "test_count = test_df[count_features].copy()\n",
    "\n",
    "# Scale the test data using the corresponding scalers\n",
    "test_count_scaled = pd.DataFrame(\n",
    "    count_scaler.transform(test_count),\n",
    "    columns=count_features,\n",
    "    index=test_count.index\n",
    ")\n",
    "test_infl_scaled = pd.DataFrame(\n",
    "    infl_scaler.transform(test_infl),\n",
    "    columns=infl_features,\n",
    "    index=test_infl.index\n",
    ")\n",
    "\n",
    "\n",
    "# Add constant term to test data\n",
    "test_count_scaled = sm.add_constant(test_count_scaled)\n",
    "test_infl_scaled = sm.add_constant(test_infl_scaled)\n",
    "\n",
    "# Ensure columns are in the same order as training data\n",
    "test_count_scaled = test_count_scaled[train_count_scaled.columns]\n",
    "test_infl_scaled = test_infl_scaled[train_infl_scaled.columns]\n",
    "\n",
    "\n",
    "#检查test_count_scaled和train_count_scaled的形状\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate predictions\n",
    "predicted_counts = result.predict(exog=test_count_scaled, exog_infl=test_infl_scaled)\n",
    "\n",
    "# Calculate probability of zero medals using predict method\n",
    "prob_zero = result.predict(exog=test_count_scaled, exog_infl=test_infl_scaled, which='prob-zero')\n",
    "\n",
    "prob_at_least_one = 1 - prob_zero\n",
    "# 将prob_at_least_one转换为DataFrame\n",
    "\n",
    "prob_at_least_one = pd.Series(prob_at_least_one, index=test_df.index)\n",
    "final_country = test_df[test_df['Never_won_medal'] == 0]['Team']\n",
    "\n",
    "# 将prob_at_least_one通过final_country进行筛选，他们的index是一样的\n",
    "prob_at_least_one = prob_at_least_one[final_country.index]\n",
    "# print(prob_at_least_one)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 最后，筛选出大于阈值的国家,并且这个国家从来没有获得过奖牌\n",
    "prediction_df = pd.DataFrame({\n",
    "    'Team': test_df['Team'],\n",
    "    'Prob_at_least_one': prob_at_least_one,\n",
    "    'Never_won_medal': test_df['Never_won_medal']\n",
    "})\n",
    "\n",
    "\n",
    "prediction_df = prediction_df[\n",
    "    # (prediction_df['Prob_at_least_one'] > threshold) & \n",
    "    (prediction_df['Never_won_medal'] == 0)\n",
    "]\n",
    "\n",
    "prediction_df = prediction_df.sort_values('Prob_at_least_one', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "## 使用蒙特卡洛模拟\n",
    "n_simulations = 10000\n",
    "simulate_count = []\n",
    "\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    # 对于每一个国家，我们都要进行一次模拟，生成伯努利随机变量（是否获奖）\n",
    "    simulate_medal = np.random.binomial(1, prob_at_least_one)\n",
    "    total_new_medal = simulate_medal.sum()\n",
    "    simulate_count.append(total_new_medal)\n",
    "    \n",
    "simulate_count = np.array(simulate_count)\n",
    "\n",
    "# 计算最可能值\n",
    "value, counts = np.unique(simulate_count, return_counts=True)\n",
    "most_likely_value = value[np.argmax(counts)]\n",
    "\n",
    "# 计算置信区间\n",
    "lower_bound = np.percentile(simulate_count, 2.5)\n",
    "upper_bound = np.percentile(simulate_count, 97.5)\n",
    "\n",
    "\n",
    "# 计算每个国家的预测命中频率\n",
    "country_simulate_hits = {\n",
    "    country: 0 for country in final_country\n",
    "}\n",
    "n_valid_simulations = 0\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    simulate_medal = np.random.binomial(1, prob_at_least_one)\n",
    "    total_new_medal = simulate_medal.sum()\n",
    "    if total_new_medal > 0:\n",
    "        n_valid_simulations += 1\n",
    "        for i, country in enumerate(final_country):\n",
    "            if simulate_medal[i] == 1:\n",
    "                country_simulate_hits[country] += 1\n",
    "            \n",
    "\n",
    "# 计算每个国家的预测命中频率\n",
    "country_simulate_hit_rate = {\n",
    "    country: hits / n_valid_simulations for country, hits in country_simulate_hits.items()\n",
    "}\n",
    "\n",
    "print(country_simulate_hit_rate)\n",
    "\n",
    "# print(\"预测命中频率：\")\n",
    "# 筛选出预测命中频率最高的国家\n",
    "predicted_countries = [country for country, rate in country_simulate_hit_rate.items() if rate > 0.2]\n",
    "print(f\"最可能首次获奖国家数量：{most_likely_value}\")\n",
    "print(f\"97.5% 置信区间：[{lower_bound:.0f}, {lower_bound:.0f}]\")\n",
    "print(\"预测可能命中国家：\", predicted_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Medal_Count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Medal_Count'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 19\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m## 开始训练零膨胀负二项模型\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 首先检查时间序列的平稳性\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_stationarity(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime Series is Stationary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 划分训练集和测试集\u001b[39;00m\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Medal_Count'"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.discrete.count_model import ZeroInflatedNegativeBinomialP\n",
    "\n",
    "\n",
    "def check_stationarity(df):\n",
    "    for col in target:\n",
    "        # ADF test\n",
    "        result = adfuller(df[col])\n",
    "        print(f'ADF Statistic for {col}: {result[0]}')\n",
    "        print(f'p-value: {result[1]}')\n",
    "        if result[1] > 0.05:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "## 开始训练零膨胀负二项模型\n",
    "\n",
    "\n",
    "# 首先检查时间序列的平稳性\n",
    "if check_stationarity(df[target]):\n",
    "    print('Time Series is Stationary')\n",
    "\n",
    "# 划分训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "formular = f'{target[0]} ~ {\" + \".join(features)}'\n",
    "\n",
    "# 训练模型\n",
    "model = ZeroInflatedNegativeBinomialP.from_formula(formular, data=X_train,exog_infl=~1)\n",
    "zinb_results = model.fit(maxiter=1000)\n",
    "\n",
    "## 预测\n",
    "y_pred = zinb_results.predict(X_test,which='prob')\n",
    "\n",
    "#计算ROC曲线\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "#寻找最佳阈值\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', label='Best Threshold')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "\n",
    "\n",
    "\n",
    "#预测下一届奥运会哪些国家第一次获得奖牌\n",
    "\n",
    "next_year = 2028\n",
    "prediction = zinb_results.predict(df[features.keys()])\n",
    "print(f'Predicted Medal Counts for {next_year} Olympics:')\n",
    "print(prediction)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二阶段使用混合预测模型\n",
    "- 首先使用ARIMAX进行时间序预测，结合历史数据和外部因素，预测下一届奥运会的奖牌数。\n",
    "- 然后结合Xgboost进行残差预测，得到最终的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## 平稳性处理模块\n",
    "def make_stationary(series):\n",
    "    d = 0 \n",
    "    while adfuller(series)[1] > 0.05:\n",
    "        series = series.diff().dropna()\n",
    "        d += 1\n",
    "    return series, d\n",
    "\n",
    "#从外部变量预处理模块\n",
    "# def prepare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
