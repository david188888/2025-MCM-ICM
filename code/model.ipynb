{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据，进行初步的EDA。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed:_0  Rank            NOC  Gold  Silver  Bronze  Total  Year  Host  \\\n",
      "0           0     1  United_States    11       7       2     20  1896     0   \n",
      "1           1     2         Greece    10      18      19     47  1896     1   \n",
      "2           2     3        Germany     6       5       2     13  1896     0   \n",
      "3           3     4         France     5       4       2     11  1896     0   \n",
      "4           4     5  Great_Britain     2       3       2      7  1896     0   \n",
      "\n",
      "   Number_of_people  ...  Cycling_Mountain_Bike  3x3_Basketball  \\\n",
      "0                14  ...                      0               0   \n",
      "1               101  ...                      0               0   \n",
      "2                19  ...                      0               0   \n",
      "3                12  ...                      0               0   \n",
      "4                10  ...                      0               0   \n",
      "\n",
      "   Cycling_BMX_Freestyle  Sport_Climbing  Marathon_Swimming,_Swimming  \\\n",
      "0                      0               0                            0   \n",
      "1                      0               0                            0   \n",
      "2                      0               0                            0   \n",
      "3                      0               0                            0   \n",
      "4                      0               0                            0   \n",
      "\n",
      "   Breaking  Cycling_Road,_Cycling_Track  Cycling_Road,_Cycling_Mountain_Bike  \\\n",
      "0         0                            0                                    0   \n",
      "1         0                            0                                    0   \n",
      "2         0                            0                                    0   \n",
      "3         0                            0                                    0   \n",
      "4         0                            0                                    0   \n",
      "\n",
      "   Cycling_Road,_Triathlon  3x3_Basketball,_Basketball  \n",
      "0                        0                           0  \n",
      "1                        0                           0  \n",
      "2                        0                           0  \n",
      "3                        0                           0  \n",
      "4                        0                           0  \n",
      "\n",
      "[5 rows x 86 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/Final_csv/result.csv')\n",
    "#删除Unnamed: 0，Rank这两列\n",
    "print(df.head())\n",
    "df = df.drop(['Unnamed:_0','Rank'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行特征筛选与特征降维\n",
    "- 首先分析变量与目标变量之间的相关性，然后分析变量与目标变量的互信息，综合考虑两者的结果，进行特征选择。\n",
    "- 观察看有没有必要进行特征降维，如果有，可以考虑使用PCA等方法进行特征降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of countries that have never won a medal: 77\n",
      "['CHA', 'NCA', 'LBA', 'PLE', 'COM', 'BRU', 'MDV', 'YAR', 'CGO', 'BEN', 'SOM', 'MLI', 'ANG', 'BAN', 'ESA', 'HON', 'SEY', 'MTN', 'SKN', 'VIN', 'LBR', 'NEP', 'PLW', 'ASA', 'SAM', 'RWA', 'MLT', 'GUI', 'BIZ', 'YMD', 'SLE', 'PNG', 'YEM', 'OMA', 'VAN', 'IVB', 'CAF', 'MAD', 'MAL', 'BIH', 'GUM', 'CAY', 'GBS', 'TLS', 'COD', 'LAO', 'ROT', 'CAM', 'SOL', 'CRT', 'GEQ', 'BOL', 'SAA', 'ANT', 'AND', 'FSM', 'MYA', 'MAW', 'RHO', 'STP', 'LIE', 'GAM', 'COK', 'SWZ', 'NBO', 'ARU', 'NRU', 'VNM', 'BHU', 'MHL', 'KIR', 'UNK', 'TUV', 'NFL', 'SSD', 'LES', 'LBN']\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "target = ['Gold', 'Silver', 'Bronze','Total']\n",
    "features = ['Rank', 'Host','Number_of_people']\n",
    "\n",
    "# # 相关性分析\n",
    "# corr = df[features+target].corr()\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()\n",
    "\n",
    "    \n",
    "# # 统计之前从来没有获得过奖牌的国家\n",
    "medal_0_country = []\n",
    "# for country in df['NOC'].unique():\n",
    "#     country_df = df[df['NOC'] == country]\n",
    "#     if country_df['Goden'].sum() == 0:\n",
    "#         medal_0_country.append(country)\n",
    "        \n",
    "# print('The number of countries that have never won a medal:', len(medal_0_country))\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('../data/summerOly_athletes.csv')\n",
    "#把Medal 这列的数据进行lable编码\n",
    "df1['Medal'] = df1['Medal'].map({'Gold': 3, 'Silver': 2, 'Bronze': 1, 'No medal': 0})\n",
    "for country in df1['NOC'].unique():\n",
    "    country_df = df1[df1['NOC'] == country]\n",
    "    #  观察Medal列，如果这个国家一直都是\"no medal\"，那么这个国家就是从来没有获得过奖牌的国家\n",
    "    if country_df['Medal'].sum() == 0:\n",
    "        medal_0_country.append(country)\n",
    "\n",
    "        \n",
    "        \n",
    "print('The number of countries that have never won a medal:', len(medal_0_country))\n",
    "print(medal_0_country)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来完成第一步也就是零膨胀模型的构建\n",
    "- 首先判断历史数据的平稳性，使用Augmented Dickey-Fuller检验验证奖牌数时间序列的平稳性。\n",
    "- 然后为第一部分（零膨胀逻辑回归）选择合适的特征，使用逻辑回归模型预测某国是否属于“永远无法获奖”的群体（结构性零值）。\n",
    "- 对于第二部分（计数部分），如果数据不平稳就选择负二项分布，如果数据平稳就选择泊松分布。\n",
    "- 最后，将两部分的结果相乘，得到最终的预测结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/python/2025-MCM-ICM/data/Final_csv/result2.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscrete\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcount_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZeroInflatedNegativeBinomialP\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df_score \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:/python/2025-MCM-ICM/data/Final_csv/result2.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#去除\"Unnamed\"列\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# df_score = df_score.loc[:, ~df_score.columns.str.contains('^Unnamed','Rank')]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(df_score.head())\u001b[39;00m\n\u001b[0;32m     10\u001b[0m train_df \u001b[38;5;241m=\u001b[39m df_score[df_score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2024\u001b[39m]\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/python/2025-MCM-ICM/data/Final_csv/result2.xlsx'"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.count_model import ZeroInflatedNegativeBinomialP\n",
    "\n",
    "# Load the data\n",
    "df_score = pd.read_excel('E:/python/2025-MCM-ICM/data/Final_csv/result2.xlsx')\n",
    "#去除\"Unnamed\"列\n",
    "# df_score = df_score.loc[:, ~df_score.columns.str.contains('^Unnamed','Rank')]\n",
    "# print(df_score.head())\n",
    "\n",
    "train_df = df_score[df_score['Year'] < 2024]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#定义模型特征，对于零膨胀特征，我们使用Never_won_medal,Total_appearance,'Athletes_growth_rate', 'Total_appearance' 'Lag_athletes', 'Lag_events'Last_appearance_athletes', 'Last_appearance_events\n",
    "# Select features with less correlation\n",
    "# Define features for inflation and count models\n",
    "infl_features  = ['Total_appearance','Lag_athletes']\n",
    "target = 'Medal_Count'\n",
    "count_features = ['People_Count','Last_appearance_athletes','Lag_athletes']\n",
    "\n",
    "# Remove duplicate features between infl_features and count_features\n",
    "count_features = [x for x in count_features if x not in infl_features]\n",
    "\n",
    "# Check correlations\n",
    "correlation_matrix = train_df[infl_features + count_features].corr()\n",
    "# print(\"Feature correlations:\\n\", correlation_matrix)\n",
    "\n",
    "# Find highly correlated feature pairs (above 0.7)\n",
    "high_correlation = correlation_matrix.abs() > 0.7\n",
    "# Get upper triangle of correlations\n",
    "high_correlation = high_correlation.where(np.triu(np.ones(high_correlation.shape), k=1).astype(bool))\n",
    "# Find feature pairs where correlation is > 0.7\n",
    "high_corr_pairs = np.where(high_correlation)\n",
    "high_corr_values = [(correlation_matrix.index[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]) \n",
    "                    for i, j in zip(*high_corr_pairs) if correlation_matrix.iloc[i, j] > 0.7]\n",
    "# print(\"\\nHighly correlated feature pairs:\")\n",
    "# for feat1, feat2, corr in high_corr_values:\n",
    "#     print(f\"{feat1} - {feat2}: {corr:.3f}\")\n",
    "# print(\"Highly correlated feature pairs:\\n\", high_correlation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create separate scalers for inflation and count features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "infl_scaler = StandardScaler()\n",
    "count_scaler = StandardScaler()\n",
    "\n",
    "# Prepare training data\n",
    "train_infl = train_df[infl_features].copy()\n",
    "train_count = train_df[count_features].copy()\n",
    "\n",
    "# Standardize features\n",
    "train_infl_scaled = infl_scaler.fit_transform(train_infl)\n",
    "train_count_scaled = count_scaler.fit_transform(train_count)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "train_infl_scaled = pd.DataFrame(train_infl_scaled, columns=infl_features, index=train_infl.index)\n",
    "train_count_scaled = pd.DataFrame(train_count_scaled, columns=count_features, index=train_count.index)\n",
    "\n",
    "train_y = train_df[target]\n",
    "\n",
    "# Add constant terms for training data\n",
    "train_count_scaled = pd.DataFrame(train_count_scaled, columns=count_features)\n",
    "train_infl_scaled = pd.DataFrame(train_infl_scaled, columns=infl_features)\n",
    "train_count_scaled = sm.add_constant(train_count_scaled)\n",
    "train_infl_scaled = sm.add_constant(train_infl_scaled)\n",
    "\n",
    "# Train ZINB model with standardized features\n",
    "zinb = ZeroInflatedNegativeBinomialP(\n",
    "    train_y,\n",
    "    train_count_scaled,\n",
    "    exog_infl=train_infl_scaled,\n",
    "    inflation='logit'\n",
    ")\n",
    "\n",
    "# Use robust optimization method\n",
    "result = zinb.fit(maxiter=10000, method='bfgs', disp=0)\n",
    "print(result.summary())\n",
    "\n",
    "#准备预测数据，使用2024年的数据\n",
    "test_df = df_score[df_score['Year'] == 2024]\n",
    "test_infl = test_df[infl_features].copy()\n",
    "test_count = test_df[count_features].copy()\n",
    "\n",
    "# Scale the test data using the corresponding scalers\n",
    "test_count_scaled = pd.DataFrame(\n",
    "    count_scaler.transform(test_count),\n",
    "    columns=count_features,\n",
    "    index=test_count.index\n",
    ")\n",
    "test_infl_scaled = pd.DataFrame(\n",
    "    infl_scaler.transform(test_infl),\n",
    "    columns=infl_features,\n",
    "    index=test_infl.index\n",
    ")\n",
    "\n",
    "\n",
    "# Add constant term to test data\n",
    "test_count_scaled = sm.add_constant(test_count_scaled)\n",
    "test_infl_scaled = sm.add_constant(test_infl_scaled)\n",
    "\n",
    "# Ensure columns are in the same order as training data\n",
    "test_count_scaled = test_count_scaled[train_count_scaled.columns]\n",
    "test_infl_scaled = test_infl_scaled[train_infl_scaled.columns]\n",
    "\n",
    "\n",
    "#检查test_count_scaled和train_count_scaled的形状\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate predictions\n",
    "predicted_counts = result.predict(exog=test_count_scaled, exog_infl=test_infl_scaled)\n",
    "\n",
    "# Calculate probability of zero medals using predict method\n",
    "prob_zero = result.predict(exog=test_count_scaled, exog_infl=test_infl_scaled, which='prob-zero')\n",
    "\n",
    "prob_at_least_one = 1 - prob_zero\n",
    "# 将prob_at_least_one转换为DataFrame\n",
    "\n",
    "prob_at_least_one = pd.Series(prob_at_least_one, index=test_df.index)\n",
    "final_country = test_df[test_df['Never_won_medal'] == 0]['Team']\n",
    "\n",
    "# 将prob_at_least_one通过final_country进行筛选，他们的index是一样的\n",
    "prob_at_least_one = prob_at_least_one[final_country.index]\n",
    "# print(prob_at_least_one)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 最后，筛选出大于阈值的国家,并且这个国家从来没有获得过奖牌\n",
    "prediction_df = pd.DataFrame({\n",
    "    'Team': test_df['Team'],\n",
    "    'Prob_at_least_one': prob_at_least_one,\n",
    "    'Never_won_medal': test_df['Never_won_medal']\n",
    "})\n",
    "\n",
    "\n",
    "prediction_df = prediction_df[\n",
    "    # (prediction_df['Prob_at_least_one'] > threshold) & \n",
    "    (prediction_df['Never_won_medal'] == 0)\n",
    "]\n",
    "\n",
    "prediction_df = prediction_df.sort_values('Prob_at_least_one', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "## 使用蒙特卡洛模拟\n",
    "n_simulations = 10000\n",
    "simulate_count = []\n",
    "\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    # 对于每一个国家，我们都要进行一次模拟，生成伯努利随机变量（是否获奖）\n",
    "    simulate_medal = np.random.binomial(1, prob_at_least_one)\n",
    "    total_new_medal = simulate_medal.sum()\n",
    "    simulate_count.append(total_new_medal)\n",
    "    \n",
    "simulate_count = np.array(simulate_count)\n",
    "\n",
    "# 计算最可能值\n",
    "value, counts = np.unique(simulate_count, return_counts=True)\n",
    "most_likely_value = value[np.argmax(counts)]\n",
    "\n",
    "# 计算置信区间\n",
    "lower_bound = np.percentile(simulate_count, 2.5)\n",
    "upper_bound = np.percentile(simulate_count, 97.5)\n",
    "\n",
    "\n",
    "# 计算每个国家的预测命中频率\n",
    "country_simulate_hits = {\n",
    "    country: 0 for country in final_country\n",
    "}\n",
    "n_valid_simulations = 0\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    simulate_medal = np.random.binomial(1, prob_at_least_one)\n",
    "    total_new_medal = simulate_medal.sum()\n",
    "    if total_new_medal > 0:\n",
    "        n_valid_simulations += 1\n",
    "        for i, country in enumerate(final_country):\n",
    "            if simulate_medal[i] == 1:\n",
    "                country_simulate_hits[country] += 1\n",
    "            \n",
    "\n",
    "# 计算每个国家的预测命中频率\n",
    "country_simulate_hit_rate = {\n",
    "    country: hits / n_valid_simulations for country, hits in country_simulate_hits.items()\n",
    "}\n",
    "\n",
    "print(country_simulate_hit_rate)\n",
    "\n",
    "# print(\"预测命中频率：\")\n",
    "# 筛选出预测命中频率最高的国家\n",
    "predicted_countries = [country for country, rate in country_simulate_hit_rate.items() if rate > 0.2]\n",
    "print(f\"最可能首次获奖国家数量：{most_likely_value}\")\n",
    "print(f\"97.5% 置信区间：[{lower_bound:.0f}, {lower_bound:.0f}]\")\n",
    "print(\"预测可能命中国家：\", predicted_countries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二阶段使用混合预测模型\n",
    "- 首先进行特征降维\n",
    "- 首先使用ARIMAX进行时间序预测，结合历史数据和外部因素，预测下一届奥运会的奖牌数。\n",
    "- 然后结合Xgboost进行残差预测，得到最终的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "from statsmodels.genmod.families import NegativeBinomial\n",
    "\n",
    "## 使得图片能够显示中文\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "df2 = pd.read_csv('../data/Final_csv/result3.csv')\n",
    "# df2 = df2.drop(columns='Total')\n",
    "# 计算每届各运动大类的子项目数量\n",
    "events_per_sport = df2.groupby(['Year', 'sport'])['event'].nunique().reset_index(name='n_Events')\n",
    "# 按国家-运动-年份统计奖牌数\n",
    "country_sport_medals = df2.groupby(['Year', 'country', 'sport']).agg({\n",
    "    'gold': 'sum',\n",
    "    'silver': 'sum',\n",
    "    'bronze': 'sum'\n",
    "}).reset_index()\n",
    "country_sport_medals['total'] = country_sport_medals[['gold', 'silver', 'bronze']].sum(axis=1)\n",
    "\n",
    "\n",
    "merged = pd.merge(country_sport_medals, events_per_sport, on=['Year', 'sport'])\n",
    "\n",
    "\n",
    "\n",
    "poisson_model = sm.GLM(merged['total'], \n",
    "                      sm.add_constant(merged[['n_Events']]), \n",
    "                      family=sm.families.Poisson()).fit()\n",
    "\n",
    "nb_model = sm.GLM(merged['total'], \n",
    "                 sm.add_constant(merged[['n_Events']]), \n",
    "                 family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# 比较AIC/BIC\n",
    "print(\"泊松模型AIC:\", poisson_model.aic)\n",
    "print(\"负二项模型AIC:\", nb_model.aic)\n",
    "\n",
    "# 过离散检验（若α>1则拒绝泊松假设）\n",
    "alpha = nb_model.params[-1]  # 负二项离散参数\n",
    "print(f\"过离散参数α={alpha:.3f} (若显著>0则需用负二项)\")\n",
    "\n",
    "# 使用面板回归模型\n",
    "model = glm(\n",
    "    formula='total ~ n_Events + C(country) + C(Year)', \n",
    "    data=merged,\n",
    "    family=NegativeBinomial(link=sm.genmod.families.links.Log())  # 使用对数连接函数\n",
    ").fit()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "# 可视化相关性，选择奖牌数量最多的前3个国家\n",
    "\n",
    "most_countries = merged.groupby('country')['total'].sum().sort_values(ascending=False).index\n",
    "print(most_countries)\n",
    "\n",
    "# 把这五个国家的数据画在一个大图上\n",
    "plt.figure(figsize=(12, 6))\n",
    "for country in most_countries:\n",
    "    country_data = merged[merged['country'] == country]\n",
    "    sns.regplot(x='n_Events', y='total', data=country_data, label=country)\n",
    "    \n",
    "#legend只写前5个国家\n",
    "plt.legend(most_countries[:10])\n",
    "plt.title('the relationship between the number of events and the number of medals')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.lmplot(x='n_Events', y='total', hue='sport', data=merged.query(\"country=='USA'\"),legend=False)\n",
    "leg = plt.legend(merged.query(\"country=='USA'\")['sport'].unique()[:10])\n",
    "leg.set_title('Sport Type', prop={'size': 12, 'weight': 'bold'})\n",
    "plt.title('USA Medal Count vs. Number of Events')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 计算各国年度总奖牌数\n",
    "country_year_total = country_sport_medals.groupby(['Year', 'country'])['total'].sum().reset_index()\n",
    "\n",
    "#合并计算各个运动占比\n",
    "hhi_data = pd.merge(\n",
    "    country_sport_medals,\n",
    "    country_year_total,\n",
    "    on=['Year', 'country'],\n",
    "    suffixes=('_sport', '_total')\n",
    ")\n",
    "hhi_data['share'] = hhi_data['total_sport'] / hhi_data['total_total']\n",
    "\n",
    "# 计算动态HHi\n",
    "def calculate_hhi(group):\n",
    "    return (group['share']**2).sum()\n",
    "\n",
    "hhi = hhi_data.groupby(['Year', 'country']).apply(calculate_hhi).reset_index(name='hhi')\n",
    "\n",
    "\n",
    "MIN_YEARS = 3  # 至少出现X届奥运会才视为核心项目\n",
    "THRESHOLD = 0.15  # 平均占比超过15%视为重要\n",
    "\n",
    "# 按运动项目聚合时间序列数据\n",
    "def analyze_core_sports(country):\n",
    "    country_data = hhi_data.query(f\"country == '{country}'\")\n",
    "    \n",
    "    # 计算各运动项目的持续表现\n",
    "    sport_stats = country_data.groupby('sport').agg(\n",
    "        avg_share=('share', 'mean'),  # 平均奖牌占比\n",
    "        max_share=('share', 'max'),    # 历史最高占比\n",
    "        n_years=('Year', 'nunique'),   # 出现届数\n",
    "        last_decade_avg=('Year', lambda x: country_data[\n",
    "            (country_data['sport'].isin(x)) & \n",
    "            (country_data['Year'] >= max(x)-12)  # 最近三届（假设每4年一届）\n",
    "        ]['share'].mean())\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 筛选核心项目标准\n",
    "    core_sports = sport_stats[\n",
    "        (sport_stats['n_years'] >= MIN_YEARS) & \n",
    "        (sport_stats['avg_share'] >= THRESHOLD)\n",
    "    ].sort_values('avg_share', ascending=False)\n",
    "    \n",
    "    return core_sports\n",
    "\n",
    "\n",
    "\n",
    "# 批量分析所有国家核心项目\n",
    "all_countries = hhi_data['country'].unique()\n",
    "core_sports_global = pd.concat(\n",
    "    [analyze_core_sports(c).assign(country=c) for c in all_countries]\n",
    ")\n",
    "\n",
    "# 生成全球核心项目榜单\n",
    "top_global = core_sports_global.groupby('sport').agg(\n",
    "    n_countries=('country', 'nunique'),\n",
    "    avg_share=('avg_share', 'mean')\n",
    ").sort_values('n_countries', ascending=False).head(10)\n",
    "\n",
    "print(\"全球最广泛核心项目:\")\n",
    "print(top_global)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 现在我想分析出美国的核心项目\n",
    "core_sports_usa = analyze_core_sports('USA')\n",
    "print(\"美国核心项目:\")\n",
    "print(core_sports_usa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保使用统一数据集并进行数据预处理\n",
    "df_host = df2.copy()\n",
    "df_host['host'] = df_host['host'].astype(int)\n",
    "# 添加一个小的常数以避免零值问题\n",
    "# 改进1：精确识别新增小项（考虑项目全称唯一性）\n",
    "def get_new_events(host_year):\n",
    "    \"\"\"识别当届首次出现的具体小项（精确到运动大类+小项）\"\"\"\n",
    "    prev_events = df_host[df_host['Year'] < host_year]['event'].unique()\n",
    "    current_events = df_host[df_host['Year'] == host_year]['event'].unique()\n",
    "    return list(set(current_events) - set(prev_events))\n",
    "\n",
    "# 改进2：完善战略项目定义\n",
    "def mark_strategic_events(host, host_year):\n",
    "    \"\"\"\n",
    "    战略新增项目定义（同时满足）：\n",
    "    1. 当届新增的具体小项（event）\n",
    "    2. 东道主在该小项的历史奖牌数 == 0（严格排除已有优势项目）\n",
    "    3. 东道主当届在该小项获得奖牌数 >= 1（降低阈值提高敏感性）\n",
    "    \"\"\"\n",
    "    strategic_events = []\n",
    "    for event in get_new_events(host_year):\n",
    "        # 历史奖牌检查（严格条件）\n",
    "        hist_medals = df_host[(df_host['country'] == host) & \n",
    "                             (df_host['event'] == event) & \n",
    "                             (df_host['Year'] < host_year)]['Total'].sum()\n",
    "        \n",
    "        # 当届奖牌检查\n",
    "        current_medals = df_host[(df_host['country'] == host) & \n",
    "                                (df_host['event'] == event) & \n",
    "                                (df_host['Year'] == host_year)]['Total'].sum()\n",
    "        \n",
    "        if hist_medals == 0 and current_medals >= 1:\n",
    "            strategic_events.append(event)\n",
    "    return strategic_events\n",
    "hosts = df_host.query(\"host == 1\")[['country', 'Year']].drop_duplicates()\n",
    "# 改进3：构建更严谨的面板数据\n",
    "\n",
    "# Improve data handling before modeling\n",
    "# 1. Add epsilon to handle zeros and adjust negative values\n",
    "analysis_data = []\n",
    "for _, row in hosts.iterrows():\n",
    "    host, host_year = row['country'], row['Year']\n",
    "    strategic_events = mark_strategic_events(host, host_year)\n",
    "    \n",
    "    # Calculate strategic medals (处理组)\n",
    "    strategic_medals = df_host[(df_host['country'] == host) & \n",
    "                             (df_host['Year'] == host_year) & \n",
    "                             (df_host['event'].isin(strategic_events))]['Total'].sum()\n",
    "    \n",
    "    # Calculate non-strategic baseline using previous years\n",
    "    base_period = [host_year-4, host_year-8]\n",
    "    non_strategic_base = df_host[(df_host['country'] == host) & \n",
    "                                (df_host['Year'].isin(base_period)) & \n",
    "                                (~df_host['event'].isin(strategic_events))]['Total'].mean()\n",
    "    \n",
    "    non_strategic_current = df_host[(df_host['country'] == host) & \n",
    "                                   (df_host['Year'] == host_year) & \n",
    "                                   (~df_host['event'].isin(strategic_events))]['Total'].sum()\n",
    "    \n",
    "    # Calculate medal changes and ensure they are non-negative\n",
    "    strategic_change = max(0.1, strategic_medals)  # Add small constant to avoid zeros\n",
    "    non_strategic_change = max(0.1, non_strategic_current - non_strategic_base)\n",
    "    \n",
    "    analysis_data.extend([\n",
    "        {\n",
    "            'host': host, \n",
    "            'Year': host_year, \n",
    "            'type': 'strategic', \n",
    "            'medal_change': strategic_change,\n",
    "            'total_medals': strategic_medals + non_strategic_current\n",
    "        },\n",
    "        {\n",
    "            'host': host, \n",
    "            'Year': host_year, \n",
    "            'type': 'non_strategic', \n",
    "            'medal_change': non_strategic_change,\n",
    "            'total_medals': strategic_medals + non_strategic_current\n",
    "        }\n",
    "    ])\n",
    "\n",
    "# Convert to DataFrame\n",
    "did_df = pd.DataFrame(analysis_data)\n",
    "\n",
    "# Try alternative model specification using Poisson regression\n",
    "model = glm(\n",
    "    formula='medal_change ~ type * C(host) + Year + total_medals',  # Add total medals control\n",
    "    data=did_df,\n",
    "    family=sm.families.Poisson()  # Use Poisson instead of Negative Binomial\n",
    ").fit()\n",
    "\n",
    "# Print model results\n",
    "print(model.summary())\n",
    "\n",
    "# Visualize results\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.boxplot(x='type', y='medal_change', data=did_df, showfliers=False)\n",
    "# plt.axhline(0, color='red', linestyle='--')\n",
    "# plt.title('Strategic vs Non-Strategic Medal Changes')\n",
    "# plt.xlabel('Event Type')\n",
    "# plt.ylabel('Medal Change')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 获取所有东道主国家及其主办年份\n",
    "\n",
    "# 存储每个国家的战略项目及其奖牌贡献\n",
    "strategic_events_impact = []\n",
    "\n",
    "for _, row in hosts.iterrows():\n",
    "    host, host_year = row['country'], row['Year']\n",
    "    strategic_events = mark_strategic_events(host, host_year)\n",
    "    \n",
    "    # 统计当届战略项目的奖牌数\n",
    "    medals = df_host[(df_host['country'] == host) & \n",
    "                    (df_host['Year'] == host_year) & \n",
    "                    (df_host['event'].isin(strategic_events))]['Total'].sum()\n",
    "    \n",
    "    # 记录结果\n",
    "    strategic_events_impact.append({\n",
    "        'country': host,\n",
    "        'Year': host_year,\n",
    "        'strategic_events': strategic_events,\n",
    "        'medals_from_strategic': medals,\n",
    "        'rate': medals / df_host[(df_host['country'] == host) & (df_host['Year'] == host_year)]['Total'].sum()\n",
    "    })\n",
    "\n",
    "# 转换为DataFrame\n",
    "strategic_impact_df = pd.DataFrame(strategic_events_impact)\n",
    "print(strategic_impact_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 使用更柔和的绿色，并保持你之前的颜色也是可以的\n",
    "sns.barplot(x='Year', y='rate', data=strategic_impact_df, palette='viridis') #  可以尝试更柔和的绿色，例如 \"#58D68D\"\n",
    "\n",
    "plt.title('Medal contribution rate of strategic events', fontsize=16)\n",
    "plt.xlabel('year', fontsize=12) #  X 轴标签使用中文更统一\n",
    "plt.ylabel('Medal contribution rate', fontsize=12) # Y 轴标签更清晰\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "\n",
    "# 3. 添加标签，调整位置和样式\n",
    "for index, row in strategic_impact_df.iterrows():\n",
    "    rate_percent = \"{:.1%}\".format(row['rate'])\n",
    "    plt.text(row['Year'],\n",
    "             row['rate'] + 0.01,  # 稍微减小向上偏移\n",
    "             rate_percent,\n",
    "             ha='center',\n",
    "             va='bottom',\n",
    "             fontsize=10,\n",
    "             color='black',\n",
    "                         bbox=dict(facecolor='black',\n",
    "                      edgecolor='none',\n",
    "                      alpha=0.7)) #  标签颜色改为黑色，更清晰\n",
    "\n",
    "#  移除背景框，如果需要可以添加更柔和的背景框\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout() #  自动调整子图参数，避免标签被裁剪\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 进行特征筛选\n",
    "# 设置可视化风格\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # 生成金牌，银牌，铜牌和总奖牌数的滞后特征\n",
    "    first_year = df['Year'].min()\n",
    "    for medal in ['Gold', 'Silver', 'Bronze', 'Total']:\n",
    "        for i in range(1, 4):\n",
    "            df[f'Lag_{medal}_{i}'] = df.groupby('NOC')[medal].shift(i)\n",
    "            df.loc[df['Year'] == first_year, f'Lag_{medal}_{i}'] = 0\n",
    "    \n",
    "        \n",
    "    # 创建主办国交互特征\n",
    "    events_col = [col for col in df.columns if 'Event' in col]\n",
    "    for event in events_col:\n",
    "        df[f'{event}_Host'] = df['Host'] * df[event]\n",
    "        \n",
    "    \n",
    "    # 检查如果滞后特征中有缺失值，我们需要填充缺失值\n",
    "    for medal in ['Gold', 'Silver', 'Bronze', 'Total']:\n",
    "        for i in range(1, 4):\n",
    "            df[f'Lag_{medal}_{i}'] = df[f'Lag_{medal}_{i}'].fillna(0)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# 特征筛选\n",
    "def feature_selection_KBEST(df,k=10,target='Total'):\n",
    "    # 选择特征\n",
    "    feature = [col for col in df.columns if col not in ['NOC', 'Year', 'Host', 'Total','Gold', 'Silver', 'Bronze']]\n",
    "    sekector = SelectKBest(f_regression, k=k)\n",
    "    X_selected = sekector.fit_transform(df[feature], df[target])\n",
    "    selected_features = df[feature].columns[sekector.get_support()]\n",
    "    #返回选择的特征的统计值,只选择前10个\n",
    "    statisc_value = pd.DataFrame({\n",
    "        'Feature': feature,\n",
    "        'Score': sekector.scores_\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    \n",
    "    return statisc_value\n",
    "def feature_selection_xgboost(df,target='Total'):\n",
    "    feature = [col for col in df.columns if col not in ['NOC', 'Year', 'Host', 'Total','Gold', 'Silver', 'Bronze']]\n",
    "    model = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "    model.fit(df[feature], df[target])\n",
    "    \n",
    "    \n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': feature,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "df = preprocess_data(df)\n",
    "# 选择特征\n",
    "selected_features = feature_selection_KBEST(df, k=10).head(13)\n",
    "print(selected_features)\n",
    "\n",
    "# 对这个selected_features进行可视化\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(data=selected_features.head(20), x='Feature', y='Score', palette='viridis')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 10 Features Selected by SelectKBest')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Host', 'Number_of_people', 'Basketball', 'Judo', 'Football',\n",
      "       'Tug-Of-War', 'Athletics', 'Swimming', 'Badminton', 'Sailing',\n",
      "       'Gymnastics', 'Art_Competitions', 'Handball', 'Weightlifting',\n",
      "       'Wrestling', 'Water_Polo', 'Hockey', 'Rowing', 'Fencing',\n",
      "       'Equestrianism', 'Shooting', 'Boxing', 'Taekwondo', 'Cycling', 'Diving',\n",
      "       'Canoeing', 'Tennis', 'Modern_Pentathlon', 'Golf', 'Softball',\n",
      "       'Archery', 'Volleyball', 'Synchronized_Swimming', 'Table_Tennis',\n",
      "       'Baseball', 'Rhythmic_Gymnastics', 'Rugby_Sevens', 'Trampolining',\n",
      "       'Beach_Volleyball', 'Triathlon', 'Rugby', 'Lacrosse', 'Polo', 'Cricket',\n",
      "       'Ice_Hockey', 'Racquets', 'Motorboating', 'Croquet', 'Figure_Skating',\n",
      "       'Jeu_De_Paume', 'Roque', 'Basque_Pelota', 'Alpinism', 'Aeronautics',\n",
      "       'Cycling_Road', 'Artistic_Gymnastics', 'Karate', 'Baseball/Softball',\n",
      "       'Trampoline_Gymnastics', 'Marathon_Swimming', 'Canoe_Slalom', 'Surfing',\n",
      "       'Canoe_Sprint', 'Cycling_BMX_Racing', 'Equestrian', 'Artistic_Swimming',\n",
      "       'Cycling_Track', 'Skateboarding', 'Cycling_Mountain_Bike',\n",
      "       '3x3_Basketball', 'Cycling_BMX_Freestyle', 'Sport_Climbing',\n",
      "       'Marathon_Swimming,_Swimming', 'Breaking',\n",
      "       'Cycling_Road,_Cycling_Track', 'Cycling_Road,_Cycling_Mountain_Bike',\n",
      "       'Cycling_Road,_Triathlon'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 393\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mprint\u001b[39m(total_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2028_pred\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# result_df.to_csv('2028_olympic_forecast.csv', index=False)\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 301\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    299\u001b[0m baseline_2024_pred \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n\u001b[1;32m--> 301\u001b[0m     baseline_model, pred, _ \u001b[38;5;241m=\u001b[39m \u001b[43marimax_forecast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcountry_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcountry_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2020\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexog_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m     baseline_2024_pred[target] \u001b[38;5;241m=\u001b[39m pred \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# 预测2028\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m, in \u001b[0;36marimax_forecast\u001b[1;34m(country_df, exog_cols, target)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 训练ARIMAX模型（注意：exog_cols不应包含is_host）\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mauto_arima\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexogenous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexog_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 确保此处只包含athletes等保留变量\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseasonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mARIMA fitting failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pmdarima\\arima\\auto.py:701\u001b[0m, in \u001b[0;36mauto_arima\u001b[1;34m(y, X, start_p, d, start_q, max_p, max_d, max_q, start_P, D, start_Q, max_P, max_D, max_Q, max_order, m, seasonal, stationary, information_criterion, alpha, test, seasonal_test, stepwise, n_jobs, start_params, trend, method, maxiter, offset_test_args, seasonal_test_args, suppress_warnings, error_action, trace, random, random_state, n_fits, return_valid_fits, out_of_sample_size, scoring, scoring_args, with_intercept, sarimax_kwargs, **fit_args)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;66;03m# init the stepwise model wrapper\u001b[39;00m\n\u001b[0;32m    670\u001b[0m     search \u001b[38;5;241m=\u001b[39m solvers\u001b[38;5;241m.\u001b[39m_StepwiseFitWrapper(\n\u001b[0;32m    671\u001b[0m         y,\n\u001b[0;32m    672\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msarimax_kwargs,\n\u001b[0;32m    699\u001b[0m     )\n\u001b[1;32m--> 701\u001b[0m sorted_res \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _return_wrapper(sorted_res, return_valid_fits, start, trace)\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pmdarima\\arima\\_auto_solvers.py:432\u001b[0m, in \u001b[0;36m_StepwiseFitWrapper.solve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m     p \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;241m<\u001b[39m max_q \u001b[38;5;129;01mand\u001b[39;00m p \u001b[38;5;241m<\u001b[39m max_p \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_k \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m--> 432\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    433\u001b[0m     q \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    434\u001b[0m     p \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pmdarima\\arima\\_auto_solvers.py:235\u001b[0m, in \u001b[0;36m_StepwiseFitWrapper._do_fit\u001b[1;34m(self, order, seasonal_order, constant)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (order, seasonal_order, constant) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_dict:\n\u001b[0;32m    231\u001b[0m \n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m# increment the number of fits\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 235\u001b[0m     fit, fit_time, new_ic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_arima\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseasonal_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# use the orders as a key to be hashed for\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# the dictionary (pointing to fit)\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_dict[(order, seasonal_order, constant)] \u001b[38;5;241m=\u001b[39m fit\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pmdarima\\arima\\_auto_solvers.py:508\u001b[0m, in \u001b[0;36m_fit_candidate_model\u001b[1;34m(y, X, order, seasonal_order, start_params, trend, method, maxiter, fit_params, suppress_warnings, trace, error_action, out_of_sample_size, scoring, scoring_args, with_intercept, information_criterion, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m fit \u001b[38;5;241m=\u001b[39m ARIMA(order\u001b[38;5;241m=\u001b[39morder, seasonal_order\u001b[38;5;241m=\u001b[39mseasonal_order,\n\u001b[0;32m    501\u001b[0m             start_params\u001b[38;5;241m=\u001b[39mstart_params, trend\u001b[38;5;241m=\u001b[39mtrend, method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    502\u001b[0m             maxiter\u001b[38;5;241m=\u001b[39mmaxiter, suppress_warnings\u001b[38;5;241m=\u001b[39msuppress_warnings,\n\u001b[0;32m    503\u001b[0m             out_of_sample_size\u001b[38;5;241m=\u001b[39mout_of_sample_size, scoring\u001b[38;5;241m=\u001b[39mscoring,\n\u001b[0;32m    504\u001b[0m             scoring_args\u001b[38;5;241m=\u001b[39mscoring_args,\n\u001b[0;32m    505\u001b[0m             with_intercept\u001b[38;5;241m=\u001b[39mwith_intercept, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m     \u001b[43mfit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# for non-stationarity errors or singular matrices, return None\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (LinAlgError, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m v:\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pmdarima\\arima\\arima.py:603\u001b[0m, in \u001b[0;36mARIMA.fit\u001b[1;34m(self, y, X, **fit_args)\u001b[0m\n\u001b[0;32m    600\u001b[0m         X \u001b[38;5;241m=\u001b[39m safe_indexing(X, \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, n_exog \u001b[38;5;241m-\u001b[39m cv))\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# Internal call\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# now make a forecast if we're validating to compute the\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;66;03m# out-of-sample score\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;66;03m# get the predictions (use self.predict, which calls forecast\u001b[39;00m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;66;03m# from statsmodels internally)\u001b[39;00m\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pmdarima\\arima\\arima.py:524\u001b[0m, in \u001b[0;36mARIMA._fit\u001b[1;34m(self, y, X, **fit_args)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    523\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 524\u001b[0m         fit, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marima_res_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    526\u001b[0m     fit, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marima_res_ \u001b[38;5;241m=\u001b[39m _fit_wrapper()\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\pmdarima\\arima\\arima.py:510\u001b[0m, in \u001b[0;36mARIMA._fit.<locals>._fit_wrapper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    507\u001b[0m _maxiter \u001b[38;5;241m=\u001b[39m fit_args\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m, _maxiter)\n\u001b[0;32m    509\u001b[0m disp \u001b[38;5;241m=\u001b[39m fit_args\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 510\u001b[0m fitted \u001b[38;5;241m=\u001b[39m \u001b[43marima\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_maxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arima, fitted\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:703\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[0;32m    701\u001b[0m         flags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhessian_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m optim_hessian\n\u001b[0;32m    702\u001b[0m     fargs \u001b[38;5;241m=\u001b[39m (flags,)\n\u001b[1;32m--> 703\u001b[0m     mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mskip_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;66;03m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_params:\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\statsmodels\\base\\model.py:566\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    565\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer()\n\u001b[1;32m--> 566\u001b[0m xopt, retvals, optim_settings \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[0;32m    576\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:243\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[1;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[0;32m    240\u001b[0m     fit_funcs\u001b[38;5;241m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[0;32m    242\u001b[0m func \u001b[38;5;241m=\u001b[39m fit_funcs[method]\n\u001b[1;32m--> 243\u001b[0m xopt, retvals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m optim_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_params\u001b[39m\u001b[38;5;124m'\u001b[39m: start_params,\n\u001b[0;32m    249\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_output\u001b[39m\u001b[38;5;124m'\u001b[39m: full_output,\n\u001b[0;32m    250\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfargs\u001b[39m\u001b[38;5;124m'\u001b[39m: fargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[0;32m    251\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretall\u001b[39m\u001b[38;5;124m'\u001b[39m: retall, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_fit_funcs\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_fit_funcs}\n\u001b[0;32m    252\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:660\u001b[0m, in \u001b[0;36m_fit_lbfgs\u001b[1;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m approx_grad:\n\u001b[0;32m    658\u001b[0m     func \u001b[38;5;241m=\u001b[39m f\n\u001b[1;32m--> 660\u001b[0m retvals \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[0;32m    666\u001b[0m     xopt, fopt, d \u001b[38;5;241m=\u001b[39m retvals\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:237\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    225\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback)\n\u001b[0;32m    226\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m: iprint,\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxcor\u001b[39m\u001b[38;5;124m'\u001b[39m: m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m: maxls}\n\u001b[1;32m--> 237\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m d \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    240\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    241\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuncalls\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    242\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    243\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarnflag\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m    244\u001b[0m f \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:297\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:267\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m--> 267\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:181\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:519\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:590\u001b[0m, in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    588\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[0;32m    589\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[1;32m--> 590\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[0;32m    592\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:470\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(x\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    468\u001b[0m     x \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(x, x0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 470\u001b[0m f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\statsmodels\\base\\model.py:534\u001b[0m, in \u001b[0;36mLikelihoodModel.fit.<locals>.f\u001b[1;34m(params, *args)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:932\u001b[0m, in \u001b[0;36mMLEModel.loglike\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    926\u001b[0m transformed, includes_fixed, complex_step, kwargs \u001b[38;5;241m=\u001b[39m _handle_args(\n\u001b[0;32m    927\u001b[0m     MLEModel\u001b[38;5;241m.\u001b[39m_loglike_param_names, MLEModel\u001b[38;5;241m.\u001b[39m_loglike_param_defaults,\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    930\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_params(params, transformed\u001b[38;5;241m=\u001b[39mtransformed,\n\u001b[0;32m    931\u001b[0m                             includes_fixed\u001b[38;5;241m=\u001b[39mincludes_fixed)\n\u001b[1;32m--> 932\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincludes_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complex_step:\n\u001b[0;32m    936\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minversion_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m INVERT_UNIVARIATE \u001b[38;5;241m|\u001b[39m SOLVE_LU\n",
      "File \u001b[1;32me:\\applist\\miniconda\\envs\\sklearn\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:1514\u001b[0m, in \u001b[0;36mSARIMAX.update\u001b[1;34m(self, params, transformed, includes_fixed, complex_step)\u001b[0m\n\u001b[0;32m   1509\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01mand\u001b[39;00m fix_any \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fix_all:\n\u001b[0;32m   1510\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot fix individual \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m parameters when\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1511\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Must either fix all \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m parameters or\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1512\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m none.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (title, condition_desc, title))\n\u001b[1;32m-> 1514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, transformed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, includes_fixed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1515\u001b[0m            complex_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;124;03m    Update the parameters of the model\u001b[39;00m\n\u001b[0;32m   1518\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;124;03m        Array of parameters.\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_params(params, transformed\u001b[38;5;241m=\u001b[39mtransformed,\n\u001b[0;32m   1536\u001b[0m                                 includes_fixed\u001b[38;5;241m=\u001b[39mincludes_fixed)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from pmdarima import auto_arima\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAXResultsWrapper\n",
    "from typing import Tuple, Dict\n",
    "import pmdarima as pm\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # 关闭警告\n",
    "\n",
    "##给df添加一个新的列，用来表示当届奥运会总项目数,除了NOC,Gold,Silver，Bronze，Total，Year，Host\n",
    "sport = df.columns[8:-1]\n",
    "df['Total_event'] = df[sport].sum(axis=1)\n",
    "def arimax_forecast(country_df, exog_cols, target):\n",
    "    \"\"\"移除is_host后的ARIMAX预测\"\"\"\n",
    "    # 数据预处理\n",
    "    country_df = country_df.sort_values('Year').reset_index(drop=True)\n",
    "    noc = country_df['NOC'].iloc[0]\n",
    "    \n",
    "    # # 检查时间连续性（奥运会每4年一届）\n",
    "    # year_diff = country_df['Year'].diff().dropna()\n",
    "    # if not (year_diff == 4).all():\n",
    "    #     print(f\"Skipping {noc}: Irregular time intervals (e.g., missing Olympics)\")\n",
    "    #     return None, None, None\n",
    "    \n",
    "    # 划分训练集（历史数据截止到2024）\n",
    "    train_data = country_df[country_df['Year'] <= 2024]\n",
    "    if len(train_data) < 2:  # 至少需要2届数据\n",
    "        return None, None, None\n",
    "    \n",
    "    # 训练ARIMAX模型（注意：exog_cols不应包含is_host）\n",
    "    try:\n",
    "        model = auto_arima(\n",
    "            train_data[target],\n",
    "            exogenous=train_data[exog_cols],  # 确保此处只包含athletes等保留变量\n",
    "            seasonal=False,\n",
    "            error_action='ignore',\n",
    "            trace=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA fitting failed for {noc}: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # 构建未来外生变量（仅保留变量如athletes）,我计划使用前3届的平均值\n",
    "    future_exog_values = train_data[exog_cols].tail(3).mean().values\n",
    "    future_exog = pd.DataFrame([future_exog_values], columns=exog_cols)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 执行预测\n",
    "    try:\n",
    "        forecast, conf_int = model.predict(\n",
    "            n_periods=1,\n",
    "            exogenous=future_exog,\n",
    "            return_conf_int=True,\n",
    "            alpha=0.05  # 95%置信区间\n",
    "        )\n",
    "\n",
    "        return model, forecast.iloc[0], conf_int\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed for {noc}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def xgb_residual_model(train_X: pd.DataFrame, train_Y: pd.Series, test_X: pd.DataFrame, n_splits: int = 3) -> Tuple[float, float]:\n",
    "    \"\"\"修复数据泄漏和特征问题的残差预测\"\"\"\n",
    "    if train_X.empty or len(train_Y) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # 确保训练数据和标签长度匹配\n",
    "    if len(train_X) != len(train_Y):\n",
    "        min_len = min(len(train_X), len(train_Y))\n",
    "        train_X = train_X.iloc[:min_len]\n",
    "        train_Y = train_Y.iloc[:min_len]\n",
    "    \n",
    "    # 动态调整交叉验证折数\n",
    "    n_splits = min(n_splits, len(train_X) - 4)  # 确保折数小于样本数\n",
    "    print(f\"the number of splits is {n_splits}\")\n",
    "    print(f\"the length of train_X is {len(train_X)}\")\n",
    "\n",
    "    if n_splits <= 2:  # 如果样本太少，无法进行交叉验证\n",
    "        print(f\"样本数量不足进行{n_splits}折交叉验证，将直接使用全部数据训练\")\n",
    "        # 使用所有数据训练一个模型\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3\n",
    "        )\n",
    "        try:\n",
    "            model.fit(train_X, train_Y)\n",
    "            pred = model.predict(test_X)[0] if len(test_X) > 0 else np.nan\n",
    "            return pred, pred\n",
    "        except Exception as e:\n",
    "            print(f\"Model training failed: {e}\")\n",
    "            return np.array([np.nan]), np.array([np.nan])\n",
    "    \"\"\"修复数据泄漏和特征问题的残差预测\"\"\"\n",
    "    if train_X.empty or len(train_Y) == 0:\n",
    "        return np.array([np.nan]), np.array([np.nan])\n",
    "    \n",
    "    # 滞后特征配置\n",
    "    lag_features = [1, 2]\n",
    "    max_lag = max(lag_features)\n",
    "    \n",
    "    # 检查训练数据是否足够生成滞后\n",
    "    if len(train_Y) < max_lag:\n",
    "        print(f\"数据不足，需要至少{max_lag}个观测值生成滞后特征\")\n",
    "        return np.array([np.nan]), np.array([np.nan])\n",
    "    \n",
    "    # 训练集滞后特征\n",
    "    for lag in lag_features:\n",
    "        train_X[f'lag{lag}'] = train_Y.shift(lag)\n",
    "    \n",
    "    # 测试集滞后特征（严格使用历史值）\n",
    "    test_lags = {f'lag{lag}': train_Y.iloc[-lag] for lag in lag_features}\n",
    "    test_X = test_X.assign(**test_lags)\n",
    "    \n",
    "    # 删除包含NaN的行（保留至少max_lag之后的数据）\n",
    "    train_X_clean = train_X.iloc[max_lag:].copy()\n",
    "    train_Y_clean = train_Y.iloc[max_lag:]\n",
    "    \n",
    "    if train_X_clean.empty:\n",
    "        return np.array([np.nan]), np.array([np.nan])\n",
    "    \n",
    "    # 重置索引确保对齐\n",
    "    train_X_clean = train_X_clean.reset_index(drop=True)\n",
    "    train_Y_clean = train_Y_clean.reset_index(drop=True)\n",
    "\n",
    "    # 添加滚动特征（窗口大小为）\n",
    "    train_X_clean['rolling_mean_2'] = train_Y_clean.rolling(window=2,closed='left',min_periods=1).mean()\n",
    "    # 删除滚动特征带来的新NaN行\n",
    "    valid_rows = train_X_clean.dropna().index\n",
    "    train_X_clean = train_X_clean.loc[valid_rows]\n",
    "    train_Y_clean = train_Y_clean.loc[valid_rows]\n",
    "\n",
    "    # 测试集滚动特征（使用训练集最后两个有效值）\n",
    "    test_X['rolling_mean_2'] = train_Y_clean.iloc[-2:].mean()\n",
    "    \n",
    "    # 时间序列交叉验证\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    test_preds = []\n",
    "    print(f\"使用{n_splits}折交叉验证训练模型\")\n",
    "    print(f\"训练集样本数: {len(train_X_clean)}\")\n",
    "    for train_idx, val_idx in tscv.split(train_X_clean):\n",
    "        # 分割数据\n",
    "        fold_train_X = train_X_clean.iloc[train_idx]\n",
    "        fold_train_Y = train_Y_clean.iloc[train_idx]\n",
    "        fold_val_X = train_X_clean.iloc[val_idx]\n",
    "        fold_val_Y = train_Y_clean.iloc[val_idx]\n",
    "        \n",
    "        # 标准化（仅在fold训练集上fit）\n",
    "        fold_scaler = StandardScaler()\n",
    "        X_train_scaled = fold_scaler.fit_transform(fold_train_X)\n",
    "        X_val_scaled = fold_scaler.transform(fold_val_X)\n",
    "        \n",
    "        # 训练模型\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_scaled, fold_train_Y)\n",
    "        \n",
    "        # 预测测试集（需标准化）\n",
    "        test_X_scaled = fold_scaler.transform(test_X)\n",
    "        test_preds.append(model.predict(test_X_scaled))\n",
    "    \n",
    "    # 最终模型训练（独立scaler）\n",
    "    final_scaler = StandardScaler()\n",
    "    X_full_scaled = final_scaler.fit_transform(train_X_clean)\n",
    "    test_X_scaled = final_scaler.transform(test_X)\n",
    "    \n",
    "    final_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ).fit(X_full_scaled, train_Y_clean)\n",
    "    \n",
    "    final_pred = final_model.predict(test_X_scaled)\n",
    "    # Ensure we return scalar values\n",
    "    mean_pred = float(np.mean(test_preds)) if test_preds else np.nan\n",
    "    final_pred_value = float(final_pred[0]) if len(final_pred) > 0 else np.nan\n",
    "    return mean_pred, final_pred_value\n",
    "\n",
    "\n",
    "def get_future_exog(country_df: pd.DataFrame, exog_cols: list, target_year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    根据历史数据生成未来外生变量预测值\n",
    "    \n",
    "    参数:\n",
    "    - country_df: 国家历史数据\n",
    "    - exog_cols: 外生变量列名列表\n",
    "    - target_year: 目标预测年份\n",
    "    \n",
    "    返回:\n",
    "    - 包含预测外生变量的DataFrame\n",
    "    \"\"\"\n",
    "    # 使用最近3届的平均值作为预测值\n",
    "    recent_data = country_df.sort_values('Year', ascending=False).head(3)\n",
    "    future_values = recent_data[exog_cols].mean()\n",
    "    \n",
    "    # 创建新的DataFrame\n",
    "    future_exog = pd.DataFrame([future_values], columns=exog_cols)\n",
    "    return future_exog\n",
    "\n",
    "def calculate_uncertainty(\n",
    "    arima_mean: float,\n",
    "    arima_conf,\n",
    "    xgb_pred: float,\n",
    "    xgb_residuals: pd.Series,  # 明确要求传入XGBoost的交叉验证残差\n",
    "    arima_residuals: pd.Series,  # ARIMA的残差（用于协方差计算）\n",
    "    use_bootstrap: bool = True  # 是否启用更精确的Bootstrap方法\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"综合不确定性计算，包含协方差和分布修正\"\"\"\n",
    "    if np.isnan(arima_mean) or np.isnan(xgb_pred):\n",
    "        return {'lower': np.nan, 'upper': np.nan}\n",
    "    \n",
    "    # 计算标准差（防御性默认值）\n",
    "    arima_std = (\n",
    "        (arima_conf[0][1] - arima_conf[0][0]) / (2 * 1.96) \n",
    "        if arima_conf is not None \n",
    "        else np.std(arima_residuals.values, ddof=1)  # 若无置信区间，用历史残差标准差\n",
    "    )\n",
    "    xgb_std = (\n",
    "        np.std(xgb_residuals, ddof=1) \n",
    "        if not xgb_residuals.empty and len(xgb_residuals) > 1 \n",
    "        else max(0.1 * abs(xgb_pred), 3)  # 防止为0\n",
    "    )\n",
    "\n",
    "    # 确保残差序列长度匹配\n",
    "    min_length = min(len(arima_residuals), len(xgb_residuals))\n",
    "    if min_length > 1:\n",
    "        # 转换为NumPy数组并截断\n",
    "        arima_arr = arima_residuals.values[:min_length]\n",
    "        xgb_arr = xgb_residuals.values[:min_length]\n",
    "        # 检查并处理可能的NaN\n",
    "        if np.isnan(arima_arr).any() or np.isnan(xgb_arr).any():\n",
    "            cov_matrix = np.array([[arima_std**2, 0], [0, xgb_std**2]])\n",
    "        else:\n",
    "            cov_matrix = np.cov(arima_arr, xgb_arr, rowvar=False)\n",
    "    else:\n",
    "        cov_matrix = np.array([[arima_std**2, 0], [0, xgb_std**2]])\n",
    "    \n",
    "    if use_bootstrap:\n",
    "        # 考虑协方差后的Bootstrap采样\n",
    "        n_samples = 1000\n",
    "        arima_samples = np.random.normal(arima_mean, arima_std, n_samples)\n",
    "        xgb_samples = np.random.multivariate_normal(\n",
    "            mean=[xgb_pred, 0],  # 假设xgb残差均值为0\n",
    "            cov=[[xgb_std**2, cov_matrix[0,1]], [cov_matrix[1,0], cov_matrix[1,1]]],\n",
    "            size=n_samples\n",
    "        )[:, 0]\n",
    "        combined = arima_samples + xgb_samples\n",
    "        lower = np.percentile(combined, 2.5)\n",
    "        upper = np.percentile(combined, 97.5)\n",
    "    else:\n",
    "        # 简单正态假设（带对数修正）\n",
    "        total_mean = arima_mean + xgb_pred\n",
    "        total_std = np.sqrt(arima_std**2 + xgb_std**2 + 2 * cov_matrix[0,1])\n",
    "        lower = max(0, total_mean - 1.96 * total_std)\n",
    "        upper = total_mean + 1.96 * total_std\n",
    "    \n",
    "    # 确保预测值非负\n",
    "    lower = max(0, lower)\n",
    "    \n",
    "    return {'lower': lower, 'upper': upper}\n",
    "           \n",
    "\n",
    "\n",
    "def main(df):\n",
    "    targets = ['Gold', 'Silver', 'Bronze', 'Total']\n",
    "    exog_cols = ['Number_of_people','Total_event']\n",
    "    final_predictions = []\n",
    "    # all_countries = df['NOC'].unique()\n",
    "    all_countries = ['United_States','China','France']\n",
    "    for noc in all_countries:\n",
    "        country_df = df[df['NOC'] == noc].sort_values('Year').reset_index(drop=True)\n",
    "        if len(country_df) == 0:\n",
    "            continue\n",
    "\n",
    "        # 获取2024年实际值（如果存在）\n",
    "        if 2024 in country_df['Year'].values:\n",
    "            actual_2024_df = country_df.loc[country_df['Year'] == 2024, targets]\n",
    "            if not actual_2024_df.empty:\n",
    "                actual_2024 = actual_2024_df.iloc[0]\n",
    "            else:\n",
    "                actual_2024 = pd.Series([np.nan]*4, index=targets)\n",
    "        else:\n",
    "            actual_2024 = pd.Series([np.nan]*4, index=targets)\n",
    "\n",
    "        # 预测2024作为基准（使用截止到2020的数据）\n",
    "        baseline_2024_pred = {}\n",
    "        for target in targets:\n",
    "            baseline_model, pred, _ = arimax_forecast(\n",
    "                country_df[country_df['Year'] <= 2020], \n",
    "                exog_cols, \n",
    "                target,\n",
    "            )\n",
    "            baseline_2024_pred[target] = pred if pred is not None else np.nan\n",
    "\n",
    "        # 预测2028\n",
    "        for target in targets:\n",
    "            print(f\"Predicting {noc} - {target}...\")\n",
    "            # ARIMAX预测\n",
    "            model, arima_pred, arima_conf = arimax_forecast(\n",
    "                country_df, \n",
    "                exog_cols, \n",
    "                target,\n",
    "            )\n",
    "            # Get ARIMA residuals from model if available\n",
    "            arima_residuals = pd.Series(model.resid) if model is not None else pd.Series([])\n",
    "            if arima_pred is None:\n",
    "                final_predictions.append({\n",
    "                    'NOC': noc, 'Target': target, \n",
    "                    '2028_pred': np.nan, '2028_lower': np.nan, '2028_upper': np.nan,\n",
    "                    '2024_actual': actual_2024.get(target, np.nan),\n",
    "                    '2024_pred': baseline_2024_pred.get(target, np.nan)\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # 准备XGBoost特征（含滞后项）\n",
    "            train_data = country_df[country_df['Year'] <= 2024].copy()\n",
    "            residuals = train_data[target] - model.predict_in_sample(\n",
    "                exogenous=train_data[exog_cols]\n",
    "            )\n",
    "            \n",
    "            train_X = train_data[exog_cols]\n",
    "\n",
    "            # XGB残差预测\n",
    "            future_exog = get_future_exog(country_df, exog_cols, 2028)\n",
    "            xgb_test = future_exog.copy()\n",
    "\n",
    "            xgb_cv_pred, xgb_full_pred = xgb_residual_model(\n",
    "                train_X, \n",
    "                residuals.astype(float), \n",
    "                xgb_test\n",
    "            )\n",
    "\n",
    "            # 处理预测值\n",
    "            if isinstance(xgb_full_pred, (np.ndarray, list)):\n",
    "                xgb_pred_value = xgb_full_pred[0] if len(xgb_full_pred) > 0 else 0\n",
    "            else:\n",
    "                xgb_pred_value = xgb_full_pred if not np.isnan(xgb_full_pred) else 0\n",
    "\n",
    "            # 综合预测与不确定性\n",
    "            combined_pred = arima_pred + xgb_pred_value\n",
    "            uncertainty = calculate_uncertainty(\n",
    "                arima_pred, arima_conf, xgb_full_pred,\n",
    "                xgb_residuals=residuals,  # 应替换为交叉验证残差\n",
    "                arima_residuals=arima_residuals\n",
    "            )\n",
    "\n",
    "            final_predictions.append({\n",
    "                'NOC': noc, 'Target': target, \n",
    "                '2028_pred': combined_pred, \n",
    "                '2028_lower': uncertainty['lower'], \n",
    "                '2028_upper': uncertainty['upper'],\n",
    "                '2024_actual': actual_2024.get(target, np.nan),\n",
    "                '2024_pred': baseline_2024_pred.get(target, np.nan)\n",
    "            })\n",
    "\n",
    "    # 后续处理与校验\n",
    "    result_df = pd.DataFrame(final_predictions)\n",
    "    result_df['improvement'] = result_df['2028_pred'] - result_df['2024_pred']\n",
    "\n",
    "    # 总奖牌数一致性检查\n",
    "    def validate_total(row):\n",
    "        if row['Target'] != 'Total':\n",
    "            return row\n",
    "        subset = result_df[(result_df['NOC'] == row['NOC']) & (result_df['Target'].isin(['Gold','Silver','Bronze']))]\n",
    "        sum_pred = subset['2028_pred'].sum()\n",
    "        if abs(row['2028_pred'] - sum_pred) > 10:\n",
    "            row['2028_pred'] = sum_pred  # 强制对齐\n",
    "            print(f\"修正 {row['NOC']} 的总奖牌数为分项之和: {sum_pred}\")\n",
    "        return row\n",
    "\n",
    "    result_df = result_df.apply(validate_total, axis=1)\n",
    "\n",
    "    # 结果展示\n",
    "    total_df = result_df\n",
    "    print(\"\\n2028年奖牌预测TOP10:\")\n",
    "    print(total_df.sort_values('2028_pred', ascending=False).head(10))\n",
    "\n",
    "    # result_df.to_csv('2028_olympic_forecast.csv', index=False)\n",
    "\n",
    "main(df)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
